{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pypots.data.generating import gene_physionet2012\n",
    "from pypots.utils.random import set_random_seed\n",
    "from pypots.optim import Adam\n",
    "from pypots.imputation import GPVAE\n",
    "from pypots.utils.metrics import cal_mae\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 16:59:18 [INFO]: Have set the random seed as 2204 for numpy and pytorch.\n",
      "2023-10-25 16:59:18 [INFO]: Loading the dataset physionet_2012 with TSDB (https://github.com/WenjieDu/Time_Series_Database)...\n",
      "2023-10-25 16:59:18 [INFO]: Starting preprocessing physionet_2012...\n",
      "2023-10-25 16:59:18 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
      "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
      "2023-10-25 16:59:18 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "2023-10-25 16:59:18 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "2023-10-25 16:59:18 [INFO]: Loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "set_random_seed()\n",
    "\n",
    "# Load the PhysioNet-2012 dataset\n",
    "physionet2012_dataset = gene_physionet2012(artificially_missing_rate=0.1)\n",
    "\n",
    "\n",
    "# Assemble the datasets for training, validating, and testing.\n",
    "\n",
    "dataset_for_training = {\n",
    "    \"X\": physionet2012_dataset['train_X'],\n",
    "}\n",
    "\n",
    "dataset_for_validating = {\n",
    "    \"X\": physionet2012_dataset['val_X'],\n",
    "    \"X_intact\": physionet2012_dataset['val_X_intact'],\n",
    "    \"indicating_mask\": physionet2012_dataset['val_X_indicating_mask'],\n",
    "}\n",
    "\n",
    "dataset_for_testing = {\n",
    "    \"X\": physionet2012_dataset['test_X'],\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(2398, 48, 37)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_for_testing[\"X\"].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 16:54:37 [INFO]: Model files will be saved to tutorial_results/imputation/gp_vae\\20231023_T165437\n",
      "2023-10-23 16:54:37 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/gp_vae\\20231023_T165437\\tensorboard\n",
      "2023-10-23 16:54:37 [INFO]: Model initialized successfully with the number of trainable parameters: 229,652\n"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "gp_vae = GPVAE(\n",
    "    n_steps=physionet2012_dataset['n_steps'],\n",
    "    n_features=physionet2012_dataset['n_features'],\n",
    "    latent_size=37,\n",
    "    encoder_sizes=(128,128),\n",
    "    decoder_sizes=(256,256),\n",
    "    kernel=\"cauchy\",\n",
    "    beta=0.2,\n",
    "    M=1,\n",
    "    K=1,\n",
    "    sigma=1.005,\n",
    "    length_scale=7.0,\n",
    "    kernel_scales=1,\n",
    "    window_size=24,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=100,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it to 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices.\n",
    "    device='cuda:0',\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/gp_vae\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 16:54:47 [INFO]: epoch 0: training loss 26157.3701, validating loss 0.5261\n",
      "2023-10-23 16:54:58 [INFO]: epoch 1: training loss 22874.5839, validating loss 0.5207\n",
      "2023-10-23 16:55:12 [INFO]: epoch 2: training loss 22839.3164, validating loss 0.5146\n",
      "2023-10-23 16:55:26 [INFO]: epoch 3: training loss 22834.6367, validating loss 0.5134\n",
      "2023-10-23 16:55:42 [INFO]: epoch 4: training loss 22826.5340, validating loss 0.5045\n",
      "2023-10-23 16:55:56 [INFO]: epoch 5: training loss 22825.2355, validating loss 0.4954\n",
      "2023-10-23 16:56:08 [INFO]: epoch 6: training loss 22823.5394, validating loss 0.4738\n",
      "2023-10-23 16:56:14 [INFO]: epoch 7: training loss 22816.4922, validating loss 0.5268\n",
      "2023-10-23 16:56:20 [INFO]: epoch 8: training loss 22819.4550, validating loss 0.4629\n",
      "2023-10-23 16:56:32 [INFO]: epoch 9: training loss 22809.6713, validating loss 0.4827\n",
      "2023-10-23 16:56:45 [INFO]: epoch 10: training loss 22807.3872, validating loss 0.4501\n",
      "2023-10-23 16:56:59 [INFO]: epoch 11: training loss 22804.0517, validating loss 0.4432\n",
      "2023-10-23 16:57:13 [INFO]: epoch 12: training loss 22803.8690, validating loss 0.4514\n",
      "2023-10-23 16:57:27 [INFO]: epoch 13: training loss 22801.9521, validating loss 0.4397\n",
      "2023-10-23 16:57:43 [INFO]: epoch 14: training loss 22800.5456, validating loss 0.4416\n",
      "2023-10-23 16:58:00 [INFO]: epoch 15: training loss 22798.7132, validating loss 0.4376\n",
      "2023-10-23 16:58:13 [INFO]: epoch 16: training loss 22797.3339, validating loss 0.4266\n",
      "2023-10-23 16:58:19 [INFO]: epoch 17: training loss 22796.5167, validating loss 0.4255\n",
      "2023-10-23 16:58:25 [INFO]: epoch 18: training loss 22797.0726, validating loss 0.4331\n",
      "2023-10-23 16:58:32 [INFO]: epoch 19: training loss 22794.4824, validating loss 0.4230\n",
      "2023-10-23 16:58:38 [INFO]: epoch 20: training loss 22794.4285, validating loss 0.4178\n",
      "2023-10-23 16:58:44 [INFO]: epoch 21: training loss 22792.3235, validating loss 0.4164\n",
      "2023-10-23 16:58:50 [INFO]: epoch 22: training loss 22792.5973, validating loss 0.4243\n",
      "2023-10-23 16:58:58 [INFO]: epoch 23: training loss 22791.0315, validating loss 0.4213\n",
      "2023-10-23 16:59:10 [INFO]: epoch 24: training loss 22791.1056, validating loss 0.4236\n",
      "2023-10-23 16:59:10 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2023-10-23 16:59:10 [INFO]: Finished training.\n",
      "2023-10-23 16:59:10 [INFO]: Saved the model to tutorial_results/imputation/gp_vae\\20231023_T165437\\GPVAE.pypots.\n",
      "2023-10-23 16:59:10 [WARNING]: ðŸš¨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mean absolute error: 0.4236\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "gp_vae.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
    "\n",
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "gp_vae_imputation = gp_vae.impute(dataset_for_testing)\n",
    "\n",
    "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "testing_mae = cal_mae(gp_vae_imputation,\n",
    "                      physionet2012_dataset['test_X_intact'], physionet2012_dataset['test_X_indicating_mask'])\n",
    "print(\"Testing mean absolute error: %.4f\" % testing_mae)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 17:00:47 [INFO]: No given device, using default device: cuda\n",
      "2023-10-23 17:00:47 [INFO]: Model files will be saved to tutorial_results/imputation/saits\\20231023_T170047\n",
      "2023-10-23 17:00:47 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/saits\\20231023_T170047\\tensorboard\n",
      "2023-10-23 17:00:47 [INFO]: Model initialized successfully with the number of trainable parameters: 1,378,358\n",
      "2023-10-23 17:00:52 [INFO]: epoch 0: training loss 0.7302, validating loss 0.3248\n",
      "2023-10-23 17:00:58 [INFO]: epoch 1: training loss 0.5149, validating loss 0.3060\n",
      "2023-10-23 17:01:03 [INFO]: epoch 2: training loss 0.4640, validating loss 0.2829\n",
      "2023-10-23 17:01:08 [INFO]: epoch 3: training loss 0.4242, validating loss 0.2663\n",
      "2023-10-23 17:01:14 [INFO]: epoch 4: training loss 0.3966, validating loss 0.2558\n",
      "2023-10-23 17:01:19 [INFO]: epoch 5: training loss 0.3760, validating loss 0.2457\n",
      "2023-10-23 17:01:24 [INFO]: epoch 6: training loss 0.3609, validating loss 0.2419\n",
      "2023-10-23 17:01:29 [INFO]: epoch 7: training loss 0.3506, validating loss 0.2363\n",
      "2023-10-23 17:01:34 [INFO]: epoch 8: training loss 0.3420, validating loss 0.2342\n",
      "2023-10-23 17:01:39 [INFO]: epoch 9: training loss 0.3352, validating loss 0.2308\n",
      "2023-10-23 17:01:44 [INFO]: epoch 10: training loss 0.3303, validating loss 0.2291\n",
      "2023-10-23 17:01:48 [INFO]: epoch 11: training loss 0.3259, validating loss 0.2278\n",
      "2023-10-23 17:01:53 [INFO]: epoch 12: training loss 0.3209, validating loss 0.2263\n",
      "2023-10-23 17:01:58 [INFO]: epoch 13: training loss 0.3173, validating loss 0.2244\n",
      "2023-10-23 17:02:04 [INFO]: epoch 14: training loss 0.3134, validating loss 0.2241\n",
      "2023-10-23 17:02:09 [INFO]: epoch 15: training loss 0.3121, validating loss 0.2232\n",
      "2023-10-23 17:02:14 [INFO]: epoch 16: training loss 0.3090, validating loss 0.2223\n",
      "2023-10-23 17:02:19 [INFO]: epoch 17: training loss 0.3086, validating loss 0.2215\n",
      "2023-10-23 17:02:25 [INFO]: epoch 18: training loss 0.3074, validating loss 0.2227\n",
      "2023-10-23 17:02:31 [INFO]: epoch 19: training loss 0.3060, validating loss 0.2209\n",
      "2023-10-23 17:02:36 [INFO]: epoch 20: training loss 0.3027, validating loss 0.2205\n",
      "2023-10-23 17:02:42 [INFO]: epoch 21: training loss 0.3016, validating loss 0.2210\n",
      "2023-10-23 17:02:47 [INFO]: epoch 22: training loss 0.3022, validating loss 0.2208\n",
      "2023-10-23 17:02:53 [INFO]: epoch 23: training loss 0.3000, validating loss 0.2205\n",
      "2023-10-23 17:02:58 [INFO]: epoch 24: training loss 0.3002, validating loss 0.2183\n",
      "2023-10-23 17:03:03 [INFO]: epoch 25: training loss 0.2973, validating loss 0.2192\n",
      "2023-10-23 17:03:08 [INFO]: epoch 26: training loss 0.2959, validating loss 0.2176\n",
      "2023-10-23 17:03:14 [INFO]: epoch 27: training loss 0.2955, validating loss 0.2159\n",
      "2023-10-23 17:03:19 [INFO]: epoch 28: training loss 0.2942, validating loss 0.2172\n",
      "2023-10-23 17:03:24 [INFO]: epoch 29: training loss 0.2946, validating loss 0.2169\n",
      "2023-10-23 17:03:29 [INFO]: epoch 30: training loss 0.2936, validating loss 0.2148\n",
      "2023-10-23 17:03:34 [INFO]: epoch 31: training loss 0.2927, validating loss 0.2177\n",
      "2023-10-23 17:03:39 [INFO]: epoch 32: training loss 0.2910, validating loss 0.2140\n",
      "2023-10-23 17:03:45 [INFO]: epoch 33: training loss 0.2898, validating loss 0.2136\n",
      "2023-10-23 17:03:50 [INFO]: epoch 34: training loss 0.2887, validating loss 0.2149\n",
      "2023-10-23 17:03:55 [INFO]: epoch 35: training loss 0.2901, validating loss 0.2150\n",
      "2023-10-23 17:04:00 [INFO]: epoch 36: training loss 0.2887, validating loss 0.2120\n",
      "2023-10-23 17:04:05 [INFO]: epoch 37: training loss 0.2929, validating loss 0.2140\n",
      "2023-10-23 17:04:11 [INFO]: epoch 38: training loss 0.2880, validating loss 0.2146\n",
      "2023-10-23 17:04:16 [INFO]: epoch 39: training loss 0.2862, validating loss 0.2133\n",
      "2023-10-23 17:04:16 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2023-10-23 17:04:16 [INFO]: Finished training.\n",
      "2023-10-23 17:04:16 [INFO]: Saved the model to tutorial_results/imputation/saits\\20231023_T170047\\SAITS.pypots.\n",
      "2023-10-23 17:04:16 [WARNING]: ðŸš¨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mean absolute error: 0.2115\n"
     ]
    }
   ],
   "source": [
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "from pypots.utils.metrics import cal_mae\n",
    "\n",
    "# initialize the model\n",
    "saits = SAITS(\n",
    "    n_steps=physionet2012_dataset['n_steps'],\n",
    "    n_features=physionet2012_dataset['n_features'],\n",
    "    n_layers=2,\n",
    "    d_model=256,\n",
    "    d_inner=128,\n",
    "    n_heads=4,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    attn_dropout=0.1,\n",
    "    diagonal_attention_mask=True,  # otherwise the original self-attention mechanism will be applied\n",
    "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "    MIT_weight=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=100,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=None,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # Set it to None to use the default device (will use CPU if you don't have CUDA devices).\n",
    "    # You can also set it to 'cpu' or 'cuda' explicitly, or ['cuda:0', 'cuda:1'] if you have multiple CUDA devices.\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/saits\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "saits.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
    "\n",
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "saits_imputation = saits.impute(dataset_for_testing)\n",
    "\n",
    "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "testing_mae = cal_mae(\n",
    "    saits_imputation, physionet2012_dataset['test_X_intact'], physionet2012_dataset['test_X_indicating_mask'])\n",
    "print(\"Testing mean absolute error: %.4f\" % testing_mae)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 17:27:48 [INFO]: Model files will be saved to tutorial_results/imputation/saits\\20231023_T172748\n",
      "2023-10-23 17:27:48 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/saits\\20231023_T172748\\tensorboard\n",
      "2023-10-23 17:27:48 [INFO]: Model initialized successfully with the number of trainable parameters: 176,882\n"
     ]
    }
   ],
   "source": [
    "from pypots.imputation import CSDI\n",
    "\n",
    "csdi = CSDI(n_layers=physionet2012_dataset['n_steps'],\n",
    "            n_features=physionet2012_dataset['n_features'],\n",
    "            d_time_embedding=1,\n",
    "            d_feature_embedding=1,\n",
    "            target_strategy = \"mix\",\n",
    "            d_diffusion_embedding = 50,\n",
    "            n_diffusion_steps= 128,\n",
    "            n_heads = 8,\n",
    "            n_channels = 8,\n",
    "            schedule = \"quad\",\n",
    "            beta_start = 0.0001,\n",
    "            beta_end = 0.5,\n",
    "            is_unconditional = True,\n",
    "            batch_size=32,\n",
    "            # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "            epochs=100,\n",
    "            device=\"cuda:0\",\n",
    "            # set the path for saving tensorboard and trained model files\n",
    "            saving_path=\"tutorial_results/imputation/saits\",\n",
    "            # only save the best model after training finished.\n",
    "            # You can also set it as \"better\" to save models performing better ever during training.\n",
    "            model_saving_strategy=\"best\",\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 17:16:33 [ERROR]: Exception: The size of tensor a (48) must match the size of tensor b (37) at non-singleton dimension 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Training got interrupted. Model was not trained. Please investigate the error printed above.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[1;32m~\\miniconda\\envs\\yaib_15\\lib\\site-packages\\pypots\\imputation\\base.py:276\u001B[0m, in \u001B[0;36mBaseNNImputer._train_model\u001B[1;34m(self, training_loader, val_loader)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m--> 276\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    277\u001B[0m \u001B[38;5;66;03m# use sum() before backward() in case of multi-gpu training\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda\\envs\\yaib_15\\lib\\site-packages\\pypots\\imputation\\csdi\\modules\\core.py:240\u001B[0m, in \u001B[0;36m_CSDI.forward\u001B[1;34m(self, inputs, training, n_sampling_times)\u001B[0m\n\u001B[0;32m    239\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_strategy \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrandom\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 240\u001B[0m     cond_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_hist_mask\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    241\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobserved_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfor_pattern_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfor_pattern_mask\u001B[49m\n\u001B[0;32m    242\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda\\envs\\yaib_15\\lib\\site-packages\\pypots\\imputation\\csdi\\modules\\core.py:108\u001B[0m, in \u001B[0;36m_CSDI.get_hist_mask\u001B[1;34m(self, observed_mask, for_pattern_mask)\u001B[0m\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# draw another sample for histmask (i-1 corresponds to another sample)\u001B[39;00m\n\u001B[1;32m--> 108\u001B[0m         cond_mask[i] \u001B[38;5;241m=\u001B[39m \u001B[43mcond_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mfor_pattern_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cond_mask\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (48) must match the size of tensor b (37) at non-singleton dimension 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mcsdi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset_for_training\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_set\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset_for_validating\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# the testing stage, impute the originally-missing values and artificially-missing values in the test set\u001B[39;00m\n\u001B[0;32m      5\u001B[0m csdi_imputation \u001B[38;5;241m=\u001B[39m csdi\u001B[38;5;241m.\u001B[39mimpute(dataset_for_testing)\n",
      "File \u001B[1;32m~\\miniconda\\envs\\yaib_15\\lib\\site-packages\\pypots\\imputation\\csdi\\model.py:235\u001B[0m, in \u001B[0;36mCSDI.fit\u001B[1;34m(self, train_set, val_set, file_type, n_sampling_times)\u001B[0m\n\u001B[0;32m    227\u001B[0m     val_loader \u001B[38;5;241m=\u001B[39m DataLoader(\n\u001B[0;32m    228\u001B[0m         val_set,\n\u001B[0;32m    229\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[0;32m    230\u001B[0m         shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    231\u001B[0m         num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_workers,\n\u001B[0;32m    232\u001B[0m     )\n\u001B[0;32m    234\u001B[0m \u001B[38;5;66;03m# Step 2: train the model and freeze it\u001B[39;00m\n\u001B[1;32m--> 235\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_train_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mload_state_dict(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_model_dict)\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39meval()  \u001B[38;5;66;03m# set the model as eval status to freeze it.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda\\envs\\yaib_15\\lib\\site-packages\\pypots\\imputation\\base.py:352\u001B[0m, in \u001B[0;36mBaseNNImputer._train_model\u001B[1;34m(self, training_loader, val_loader)\u001B[0m\n\u001B[0;32m    350\u001B[0m logger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mException: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbest_model_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 352\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    353\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining got interrupted. Model was not trained. Please investigate the error printed above.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    354\u001B[0m     )\n\u001B[0;32m    355\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    356\u001B[0m     \u001B[38;5;167;01mRuntimeWarning\u001B[39;00m(\n\u001B[0;32m    357\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining got interrupted. Please investigate the error printed above.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    358\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel got trained and will load the best checkpoint so far for testing.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    359\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf you don\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt want it, please try fit() again.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    360\u001B[0m     )\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Training got interrupted. Model was not trained. Please investigate the error printed above."
     ]
    }
   ],
   "source": [
    "\n",
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "csdi.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
    "\n",
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "csdi_imputation = csdi.impute(dataset_for_testing)\n",
    "\n",
    "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "testing_mae = cal_mae(\n",
    "    csdi_imputation, physionet2012_dataset['test_X_intact'], physionet2012_dataset['test_X_indicating_mask'])\n",
    "print(\"Testing mean absolute error: %.4f\" % testing_mae)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 17:19:40 [INFO]: Model files will be saved to tutorial_results/imputation/us_gan\\20231023_T171940\n",
      "2023-10-23 17:19:40 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/us_gan\\20231023_T171940\\tensorboard\n",
      "2023-10-23 17:19:40 [INFO]: Model initialized successfully with the number of trainable parameters: 1,258,517\n",
      "2023-10-23 17:21:21 [INFO]: epoch 0: training loss_generator 4.0457, train loss_discriminator 0.1884\n",
      "2023-10-23 17:23:06 [INFO]: epoch 1: training loss_generator 4.8104, train loss_discriminator 0.1214\n",
      "2023-10-23 17:24:33 [INFO]: epoch 2: training loss_generator 5.2982, train loss_discriminator 0.0933\n",
      "2023-10-23 17:27:23 [INFO]: epoch 3: training loss_generator 5.6700, train loss_discriminator 0.0778\n",
      "2023-10-23 17:27:23 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2023-10-23 17:27:23 [INFO]: Finished training.\n",
      "2023-10-23 17:27:23 [INFO]: Saved the model to tutorial_results/imputation/us_gan\\20231023_T171940\\USGAN.pypots.\n",
      "2023-10-23 17:27:23 [WARNING]: ðŸš¨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mean absolute error: 0.2691\n"
     ]
    }
   ],
   "source": [
    "from pypots.optim import Adam\n",
    "from pypots.imputation import USGAN\n",
    "from pypots.utils.metrics import cal_mae\n",
    "\n",
    "# initialize the model\n",
    "us_gan = USGAN(\n",
    "    n_steps=physionet2012_dataset['n_steps'],\n",
    "    n_features=physionet2012_dataset['n_features'],\n",
    "    rnn_hidden_size=256,\n",
    "    lambda_mse=1,\n",
    "    dropout_rate=0.1,\n",
    "    G_steps=1,\n",
    "    D_steps=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=100,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    G_optimizer=Adam(lr=1e-3),\n",
    "    D_optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it to 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices.\n",
    "    device='cuda:0',\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/us_gan\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")\n",
    "\n",
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "us_gan.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
    "\n",
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "us_gan_imputation = us_gan.impute(dataset_for_testing)\n",
    "\n",
    "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "testing_mae = cal_mae(us_gan_imputation,\n",
    "                      physionet2012_dataset['test_X_intact'], physionet2012_dataset['test_X_indicating_mask'])\n",
    "print(\"Testing mean absolute error: %.4f\" % testing_mae)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   person_id value\n",
      "0          1     A\n",
      "1          1     B\n",
      "2          2     C\n",
      "3          2     D\n",
      "4          2     E\n",
      "5          3     F\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'person_id': [1, 1, 2, 2, 2, 3],\n",
    "        'value': ['A', 'B', 'C', 'D', 'E', 'F']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Identify the maximum number of rows for a person\n",
    "max_rows = df.groupby('person_id').size().max()\n",
    "\n",
    "# Step 2 and 3: Create a new DataFrame with the maximum number of rows for each person\n",
    "max_rows_df = (df.groupby('person_id', group_keys=False)\n",
    "               .apply(lambda group: group.head(max_rows))\n",
    "               .reset_index(drop=True))\n",
    "\n",
    "# Display the result\n",
    "print(max_rows_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  person_id value\n",
      "0         1     A\n",
      "1         1     B\n",
      "2         2     C\n",
      "3         2     D\n",
      "4         2     E\n",
      "5         3     F\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'person_id': [1, 1, 2, 2, 2, 3],\n",
    "        'value': ['A', 'B', 'C', 'D', 'E', 'F']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Identify the maximum number of rows for a person\n",
    "max_rows = df.groupby('person_id').size().max()\n",
    "\n",
    "# Step 2: Create a new DataFrame with the maximum number of rows for each person\n",
    "max_rows_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "# Step 3: Copy data from the original DataFrame to the new DataFrame\n",
    "for person_id, group in df.groupby('person_id'):\n",
    "    rows_to_copy = min(len(group), max_rows)\n",
    "    max_rows_df = pd.concat([max_rows_df, group.head(rows_to_copy)])\n",
    "\n",
    "# Display the result\n",
    "print(max_rows_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   person_id value\n",
      "0          1     A\n",
      "1          1     B\n",
      "2          2     C\n",
      "3          2     D\n",
      "4          2     E\n",
      "5          3     F\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Robin\\AppData\\Local\\Temp\\ipykernel_29968\\170729978.py:13: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  .apply(lambda group: group.head(max_rows))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'person_id': [1, 1, 2, 2, 2, 3],\n",
    "        'value': ['A', 'B', 'C', 'D', 'E', 'F']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 1: Identify the maximum number of rows for a person\n",
    "max_rows = df['person_id'].value_counts().max()\n",
    "\n",
    "# Step 2 and 3: Create a new DataFrame with the maximum number of rows for each person\n",
    "max_rows_df = (df.groupby('person_id')\n",
    "               .apply(lambda group: group.head(max_rows))\n",
    "               .reset_index(drop=True))\n",
    "\n",
    "# Display the result\n",
    "print(max_rows_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   person_id value\n",
      "0          1     A\n",
      "1          1     B\n",
      "2          1  None\n",
      "3          2     C\n",
      "4          2     D\n",
      "5          2     E\n",
      "6          3     F\n",
      "7          3  None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'person_id': [1, 1, 2, 2, 2, 3],\n",
    "        'value': ['A', 'B', 'C', 'D', 'E', 'F']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Identify the maximum number of rows for a person\n",
    "max_rows = df['person_id'].value_counts().max()\n",
    "\n",
    "# Create a new DataFrame with the maximum number of rows for each person\n",
    "max_rows_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each person_id and append rows until reaching the maximum\n",
    "for person_id, group in df.groupby('person_id'):\n",
    "    num_rows = len(group)\n",
    "    if num_rows < max_rows:\n",
    "        diff = max_rows - num_rows\n",
    "        rows_to_add = group.iloc[:diff].copy()\n",
    "        rows_to_add['value'] = None  # You can fill this with your desired value\n",
    "        max_rows_df = pd.concat([max_rows_df, group, rows_to_add], ignore_index=True)\n",
    "    else:\n",
    "        max_rows_df = pd.concat([max_rows_df, group], ignore_index=True)\n",
    "\n",
    "# Display the result\n",
    "print(max_rows_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# m = len(max(dfs, key=len))\n",
    "values= df.reindex(range(10), fill_value=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "   person_id value\n0          1     A\n1          1     B\n2          2     C\n3          2     D\n4          2     E\n5          3     F\n6          0     0\n7          0     0\n8          0     0\n9          0     0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>person_id</th>\n      <th>value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>B</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>E</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "np.reshape(train_dataset.amputated_values.values, (-1, train_dataset.maxlen, train_dataset.features_df.shape[1]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[   0    1    2    3    4    5]\n",
      "  [   6    7    8    9   10   11]\n",
      "  [  12   13   14   15   16   17]\n",
      "  ...\n",
      "  [ 990  991  992  993  994  995]\n",
      "  [ 996  997  998  999 1000 1001]\n",
      "  [1002 1003 1004 1005 1006 1007]]\n",
      "\n",
      " [[1008 1009 1010 1011 1012 1013]\n",
      "  [1014 1015 1016 1017 1018 1019]\n",
      "  [1020 1021 1022 1023 1024 1025]\n",
      "  ...\n",
      "  [1998 1999 2000 2001 2002 2003]\n",
      "  [2004 2005 2006 2007 2008 2009]\n",
      "  [2010 2011 2012 2013 2014 2015]]]\n"
     ]
    }
   ],
   "source": [
    "# Creating an array with 12 elements\n",
    "arr = np.arange(2016)\n",
    "\n",
    "# Reshaping the array to a 2x6 matrix\n",
    "reshaped_arr = arr.reshape(-1, 168,6)\n",
    "\n",
    "print(reshaped_arr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": [100, 200, 200, 300, 300, 300], \"val1\": [1.5, 2.5, 4.5, np.nan, 6.5, np.nan], \"val2\": [9.5, 7.5, 8.5, 3.5, np.nan, np.nan]})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "    id  val1  val2\n0  100   1.5   9.5\n1  200   2.5   7.5\n2  200   4.5   8.5\n3  300   NaN   3.5\n4  300   6.5   NaN\n5  300   NaN   NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>val1</th>\n      <th>val2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>1.5</td>\n      <td>9.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>200</td>\n      <td>2.5</td>\n      <td>7.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>200</td>\n      <td>4.5</td>\n      <td>8.5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>300</td>\n      <td>NaN</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>300</td>\n      <td>6.5</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>300</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "     val1  val2\nid             \n100   1.5   9.5\n100   0.0   0.0\n100   0.0   0.0\n200   2.5   7.5\n200   4.5   8.5\n200   0.0   0.0\n300   NaN   3.5\n300   6.5   NaN\n300   NaN   NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>val1</th>\n      <th>val2</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>100</th>\n      <td>1.5</td>\n      <td>9.5</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>2.5</td>\n      <td>7.5</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>4.5</td>\n      <td>8.5</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>NaN</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>6.5</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['new']=df.groupby('id').cumcount()\n",
    "df_true=df.set_index(['id','new']).unstack(fill_value=0).stack(dropna=False).reset_index('id').set_index('id')\n",
    "df_true"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "df_true.reset_index(inplace=True,drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "     val1  val2\nid             \n100   1.5   9.5\n100   0.0   0.0\n100   0.0   0.0\n200   2.5   7.5\n200   4.5   8.5\n200   0.0   0.0\n300   NaN   3.5\n300   6.5   NaN\n300   NaN   NaN",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>val1</th>\n      <th>val2</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>100</th>\n      <td>1.5</td>\n      <td>9.5</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>2.5</td>\n      <td>7.5</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>4.5</td>\n      <td>8.5</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>NaN</td>\n      <td>3.5</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>6.5</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>300</th>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_true.set_index(\"id\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
