import gin.torch.external_configurables
import icu_benchmarks.models.wrappers
import icu_benchmarks.models.encoders
import icu_benchmarks.models.utils

default_preprocessor.generate_features = False

# Train params
train_common.model = @DLWrapper()

DLWrapper.encoder = @TemporalConvNet()
DLWrapper.optimizer_fn = @Adam

DLWrapper.train.epochs = 1000
DLWrapper.train.batch_size = 64
DLWrapper.train.patience = 10
DLWrapper.train.min_delta = 1e-4

# Optimizer params
optimizer/hyperparameter.class_to_tune = @Adam
optimizer/hyperparameter.weight_decay = 1e-6
optimizer/hyperparameter.lr = (1e-5, 3e-4)

# Encoder params
model/hyperparameter.class_to_tune = @TemporalConvNet
model/hyperparameter.num_classes = %NUM_CLASSES
model/hyperparameter.max_seq_length = %HORIZON
model/hyperparameter.num_channels = (32, 256, "log-uniform", 2)
model/hyperparameter.kernel_size = (2, 32, "log-uniform", 2)
model/hyperparameter.dropout = (0.0, 0.4)

tune_hyperparameters.scopes = ["model", "optimizer"]
tune_hyperparameters.n_initial_points = 5
tune_hyperparameters.n_calls = 30
tune_hyperparameters.folds_to_tune_on = 2